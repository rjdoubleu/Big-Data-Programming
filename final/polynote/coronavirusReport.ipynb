{
  "metadata" : {
    "config" : {
      "dependencies" : {
        
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.master" : "local[*]"
      }
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# Coronavirus Daily Report\n",
        "\n",
        "\n",
        "Running this notebook every day updates the cases and deaths report by country/region and reports the Pearson correlation. This notebook utilizes the Spark SQL and Spark MLlib frameworks.\n",
        "\n",
        "### Import Packages\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1588310003656,
          "endTs" : 1588310004687
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import sys.process._\n",
        "import spark.{sparkContext => sc}\n",
        "import org.apache.spark.sql.types._"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 2,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "## Dataset Formation\n",
        "\n",
        "### Downloading the Kaggle Datasets using Kaggle API\n",
        "\n",
        "\n",
        "Use bash to access the Kaggle API and download the datasets. To use the API I had to download my account's kaggle.json file and move it into ~/.kaggle. Below we use bash commands to perform the necessary operations.\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1588310004720,
          "endTs" : 1588310007590
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "\"kaggle datasets files sudalairajkumar/novel-corona-virus-2019-dataset\" !"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "name                                     size  creationDate         \n",
            "-------------------------------------  ------  -------------------  \n",
            "COVID19_open_line_list.csv                3MB  2020-04-30 05:26:54  \n",
            "covid_19_data.csv                         1MB  2020-04-30 05:26:54  \n",
            "time_series_covid_19_confirmed_US.csv  1013KB  2020-04-30 05:26:54  \n",
            "time_series_covid_19_deaths.csv          65KB  2020-04-30 05:26:54  \n",
            "time_series_covid_19_deaths_US.csv      974KB  2020-04-30 05:26:54  \n",
            "COVID19_line_list_data.csv              359KB  2020-04-30 05:26:54  \n",
            "time_series_covid_19_recovered.csv       70KB  2020-04-30 05:26:54  \n",
            "time_series_covid_19_confirmed.csv       84KB  2020-04-30 05:26:54  \n"
          ],
          "output_type" : "stream"
        },
        {
          "execution_count" : 3,
          "data" : {
            "text/plain" : [
              "0"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 4,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "For this report, we need to download two of the CSV files and later join them using SQL.\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1588310007597,
          "endTs" : 1588310009397
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "\"kaggle datasets download sudalairajkumar/novel-corona-virus-2019-dataset -f time_series_covid_19_confirmed.csv -p /home/waldenr1_gmail_com/polynote/notebooks/finalProject/data\" !"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "time_series_covid_19_confirmed.csv: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "output_type" : "stream"
        },
        {
          "execution_count" : 5,
          "data" : {
            "text/plain" : [
              "0"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1588310009413,
          "endTs" : 1588310011183
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "\"kaggle datasets download sudalairajkumar/novel-corona-virus-2019-dataset -f time_series_covid_19_deaths.csv -p /home/waldenr1_gmail_com/polynote/notebooks/finalProject/data\" !"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "time_series_covid_19_deaths.csv: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "output_type" : "stream"
        },
        {
          "execution_count" : 6,
          "data" : {
            "text/plain" : [
              "0"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 7,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "### Loading the Dataset\n",
        "\n",
        "\n",
        "Here I read the .csv files into separate Spark DataFrames. Since these files are only updated daily they can be treated as static so I allow for the schema to first be inferred. After initial inference, I cast the columns values to SQL DataTypes by programmatically enforcing a custom schema.\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 8,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1588310011194,
          "endTs" : 1588310024146
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val path = \"/home/waldenr1_gmail_com/polynote/notebooks/finalProject/data/\"\n",
        "\n",
        "val test_df = spark\n",
        "                .read\n",
        "                .format(\"csv\")\n",
        "                .option(\"header\", \"true\")\n",
        "                .csv(path + \"time_series_covid_19_deaths.csv\")\n",
        "\n",
        "val schemaUpdate = StructType(\n",
        "                test_df.columns.slice(0,2).map(StructField(_ , StringType, true)) ++ \n",
        "                test_df.columns.slice(2,4).map(StructField(_ , DoubleType, true)) ++ \n",
        "                test_df.columns.drop(4).map(StructField(_ , IntegerType, true))\n",
        "                )\n",
        "\n",
        "val confirmed_df = spark\n",
        "                    .read\n",
        "                    .format(\"csv\")\n",
        "                    .option(\"header\", \"true\")\n",
        "                    .schema(schemaUpdate)\n",
        "                    .csv(path + \"time_series_covid_19_confirmed.csv\")\n",
        "\n",
        "val deaths_df = spark\n",
        "                    .read\n",
        "                    .format(\"csv\")\n",
        "                    .option(\"header\", \"true\")\n",
        "                    .schema(schemaUpdate)\n",
        "                    .csv(path + \"time_series_covid_19_deaths.csv\")"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 9,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "### Preparing Dataset for SQL Interpretation\n",
        "\n",
        "The .csv files I downloaded are not SQL friendly due to columns containing the \"/\" character. By replacing all of the \"/\" characters with \"_\" characters, the DataFrames can be interpreted by SQL. Lastly, I use scala to grab the last column name which holds the newest data (this operation cannot be performed in SQL).\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 10,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1588310024176,
          "endTs" : 1588310025235
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val newCols = deaths_df.columns.map(_.replace(\"/\", \"_\"))\n",
        "val deaths_df_newCols = deaths_df.toDF(newCols:_*)\n",
        "val confirmed_df_newCols = confirmed_df.toDF(newCols:_*)\n",
        "val newest = deaths_df_newCols.columns.last"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 11,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "### Make Data Visible to SQLÂ \n",
        "\n",
        "The following cell simply exposes the DataFrames to the SQL interpreter as \"deaths\" and \"confirmed\" respectively.\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 12,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1588310025265,
          "endTs" : 1588310032908
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "deaths_df_newCols.createGlobalTempView(\"deaths\")\n",
        "confirmed_df_newCols.createGlobalTempView(\"confirmed\")"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 13,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "## Generating the Daily Report (Spark SQL)\n",
        "\n",
        "\n",
        "Here we form the Daily Report\n",
        "\n",
        "\n",
        "### Join and Query\n",
        "\n",
        "This SQL query sums the deaths and cases for each country/region in the dataset for the most recent day.\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 14,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1588310032920,
          "endTs" : 1588310033540
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val agg_df = spark.sql( \" SELECT deaths.Country_Region AS Country_Region,\" + \n",
        "                        \" SUM(confirmed.\" + newest + \") AS Cases,\" +\n",
        "                        \" SUM(deaths.\" + newest + \") AS Deaths\" +\n",
        "                        \" FROM global_temp.deaths deaths\" +\n",
        "                        \" JOIN global_temp.confirmed confirmed\" +\n",
        "                        \" ON deaths.Country_Region = confirmed.Country_Region\" +\n",
        "                        \" GROUP BY deaths.Country_Region\" +\n",
        "                        \" ORDER BY Cases DESC\"\n",
        "                        )"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 15,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "### Summarizing the Results\n",
        "\n",
        "\n",
        "Here we summarize the results from our SQL query output.\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 16,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1588310033544,
          "endTs" : 1588310042751
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val summary_df = agg_df.describe(\"Cases\", \"Deaths\")"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 17,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "### Calculate Deaths Pearson Correlation (Spark MLlib)Â \n",
        "\n",
        "Here we use the VectorAssembler function to create a feature vector column from a multicolumn DataFrame, then calculate the Pearson correlation between the Cases and Deaths.\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 18,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1588310042761,
          "endTs" : 1588310052662
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import org.apache.spark.ml.feature.VectorAssembler\n",
        "import org.apache.spark.ml.stat.Correlation\n",
        "\n",
        "val assembler = new VectorAssembler()\n",
        "                    .setInputCols(Array(\"Cases\", \"Deaths\"))\n",
        "                    .setOutputCol(\"features\")\n",
        "\n",
        "val features = assembler.transform(agg_df)\n",
        "\n",
        "val pearson_corr_mat = Correlation.corr(features, \"features\").head"
      ],
      "outputs" : [
      ]
    }
  ]
}